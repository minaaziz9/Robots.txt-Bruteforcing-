ğŸ”¹ What Happens at Each Step?
1ï¸âƒ£ Gets robots.txt from all live subdomains â†’ Saves results in robots_results.txt.
2ï¸âƒ£ Extracts Disallow paths and saves them in robots_disallowed.txt.
3ï¸âƒ£ Uses ffuf to test directories inside robots.txt paths using a good SecLists wordlist.
4ï¸âƒ£ Uses dirsearch to look for real files in restricted directories.

ğŸš€ After this, check results for sensitive files or admin panels and move to exploitation!
